# Sample Logstash configuration for creating a simple
# Beats -> Logstash -> Elasticsearch pipeline.

input {
  jdbc {
    clean_run => true
	jdbc_driver_library => "C:\Program Files (x86)\MySQL\Connector J 8.0\mysql-connector-java-8.0.19.jar"
	jdbc_driver_class => "com.mysql.jdbc.Driver"
	jdbc_connection_string => "jdbc:mysql://localhost:3306/project_alpha"
	jdbc_user => "root"
	jdbc_password => "root"
	schedule => "* * * * *"
	statement => "SELECT user_id as id,languages,middlewares,databasesList,operating_system,experience,frameworks,i.cus_name,i.cus_addr,i.cus_dob,i.cus_email,i.cus_tel FROM user_cv INNER JOIN user_info i ON user_cv.user_id = i.CUS_USER_ID WHERE user_cv.user_id > :sql_last_value"
	use_column_value => true
	tracking_column => "id"
  }
}

filter {


}

output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "user_info_index"
	#Below I have told plugin to use the primary key (id field) in the user_cv table as the unique document id. 
	#Then Elasticsearch will not create multiple documents for the same record 
	#since it does not create documents with same document id. 
	#Then the existing one will get replaced with the newest ones.
	#Then the newly added and changed records data will be present at the Elasticsearch.
	document_type => "user_info"
	document_id => "%{id}"
    #user => "elastic"
    #password => "changeme"
  }
  
  stdout {
	codec => rubydebug
  }
}
